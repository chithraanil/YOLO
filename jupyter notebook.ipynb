{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection On Images using YOLO-V3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1:\n",
    "1. Import cv2 and numpy libraries. \n",
    "2. Then we load yolo v3 algorithm using cv2.dnn.readNet by passing weights and cfg file. \n",
    "3. Then we will load all classes names in array using coco.names file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load YOLO\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\",\"yolov3.cfg\")\n",
    "classes = []\n",
    "with open(\"coco.names\",\"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### These are the 80 labels or categories YOLO V3 can identify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2:\n",
    "- Next we will define output layers because thatâ€™s where we will be defining what object is detected by using __net.getUnconnectedOutLayers__ and __net.getLayerNames__.\n",
    "\n",
    "- __getUnconnectedOutLayers() __: gives the names of the unconnected output layers, which are essentially the last layers of the network. \n",
    "- Then we run the forward pass of the network to get output from the output layers, as you will see  __(net.forward(getOutputsNames(net)))__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.getLayerNames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.getUnconnectedOutLayers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = net.getLayerNames()\n",
    "outputlayers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputlayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors= np.random.uniform(0,255,size=(len(classes),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3:\n",
    "- Next let us load an image. We will reduce the height and width of our image to scale of 40% and 30%. And save all those values in height,width,channels variables for theoriginal image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading image\n",
    "img = cv2.imread(\"demo1.jpg\")\n",
    "img = cv2.resize(img,None,fx=0.4,fy=0.3)\n",
    "height,width,channels = img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's view the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# To know the shape of images\n",
    "def image_and_shapes(image):\n",
    "    img= plt.imread(image)\n",
    "    plt.imshow(img)\n",
    "    print(\"Shape of the image:{}\".format(img.shape))\n",
    "\n",
    "image_and_shapes(\"demo1.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is our original image from which we want to detect as many objects as possible. But we cannot give this image directly to algorithm.So we need to do some conversion from this image. This is called blob conversion which is basically extracting features from image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __We will detect objects in blob by using cv2.dnn.blobFromImage and passing few variables: img is file name, scalefactor of 0.00392, size of image to be used in blob be (416,416), no mean subtraction from layers as (0,0,0), setting True flag means we will be inverting blue with red since OpenCV uses BGR but we have channels in image as RGB.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cv.dnn.blobFromImage(img, scale, size, mean)\n",
    "- __blob = cv.dnn.blobFromImage(img, 1/255.0, (416, 416), swapRB=True, crop=False)__\n",
    "- the image to transform\n",
    "- the scale factor (1/255 to scale the pixel values to [0..1])\n",
    "- the size, here a 416x416 square image\n",
    "- the mean value (default=0)\n",
    "- the option swapBR=True (since OpenCV uses BGR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#detecting objects\n",
    "blob = cv2.dnn.blobFromImage(img,0.00392,(416,416),(0,0,0),True,crop=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We now pass this blob to network using __net.setInput(blob)__ and then forward this to the __outputlayers__. Here all objects have been detected and outs contains all the information we need to instruct to extract the position of the object like top,left,right,bottom positions,name of class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.setInput(blob)\n",
    "outs = net.forward(outputlayers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's evaluate outs by showing information on screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Showing info on screen/ get confidence score of algorithm in detecting an object in blob\n",
    "class_ids=[]\n",
    "confidences=[]\n",
    "boxes=[]\n",
    "for out in outs:\n",
    "    for detection in out:\n",
    "        scores = detection[5:]\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "        \n",
    "        \n",
    "        if confidence > 0.5:\n",
    "            #onject detected\n",
    "            center_x= int(detection[0]*width)\n",
    "            center_y= int(detection[1]*height)\n",
    "            w = int(detection[2]*width)\n",
    "            h = int(detection[3]*height)\n",
    "        \n",
    "            #cv2.circle(img,(center_x,center_y),10,(0,255,0),2)\n",
    "            #rectangle co-ordinaters\n",
    "            x=int(center_x - w/2)\n",
    "            y=int(center_y - h/2)\n",
    "            #cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "            \n",
    "            boxes.append([x,y,w,h]) #put all rectangle areas\n",
    "            confidences.append(float(confidence)) #how confidence was that object detected and show that percentage\n",
    "            class_ids.append(class_id) #name of the object tha was detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There might be cases that multiple time the same object might be detected like below.  We want to eliminate this.\n",
    "- To eliminate this, we will use Non-Max Suppression(NMS) functionality. What this will do is eliminate the boxes by using some threshold value(any box having value less than 0.6- that will be removed) and it determines that keep only the best of all boxes. And indexes variable will keep track of such unique objects detected. So no multiple detection of same objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv2.dnn.NMSBoxes(boxes,confidences,score_threshold,IOU_threshold)\n",
    "indexes = cv2.dnn.NMSBoxes(boxes,confidences,0.4,0.6)#Non Max Suppressions\n",
    "\n",
    "'''Now using below loop over all found boxes, \n",
    "if box is appearing in indexes then only draw rectangle, color it,\n",
    "put text of class name on it.'''\n",
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "for i in range(len(boxes)):\n",
    "    if i in indexes:\n",
    "        x,y,w,h = boxes[i]\n",
    "        label = str(classes[class_ids[i]])\n",
    "        color = colors[i]\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),color,2)\n",
    "        cv2.putText(img,label,(x,y+30),font,1,(255,255,255),2)\n",
    "            \n",
    "cv2.imshow(\"Image\",img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
